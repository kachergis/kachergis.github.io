<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: October 3, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.c0e352886d22860db4df90f1df196e9c.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" disabled>
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" >
  

  
  



























  
  
  






  <meta name="author" content="George Kachergis" />





  

<meta name="description" content="Many everyday activities are sequential in nature. That is, they can be seen as a sequence of subactions and sometimes subgoals. In the motor execution of sequential action, context effects are observed in which later subactions modulate the execution of earlier subactions (e.g., reaching for an overturned mug, people will optimize their grasp to achieve a comfortable end state). A trajectory (movement) adaptation of an often-used paradigm in the study of sequential action, the serial response time task, showed several context effects of which centering behavior is of special interest. Centering behavior refers to the tendency (or strategy) of subjects to move their arm or mouse cursor to a position equidistant to all stimuli in the absence of predictive information, thereby reducing movement time to all possible targets. In the current study, we investigated sequential action in a virtual robotic agent trained using proximal policy optimization, a state-of-the-art deep reinforcement learning algorithm. The agent was trained to reach for appearing targets, similar to a serial response time task given to humans. We found that agents were more likely to develop centering behavior similar to human subjects after curricularized learning. In our curriculum, we first rewarded agents for reaching targets before introducing a penalty for energy expenditure. When the penalty was applied with no curriculum, many agents failed to learn the task due to a lack of action space exploration, resulting in high variability of agents&#39; performance. Our findings suggest that in virtual agents, similar to infants, early energetic exploration can promote robust later learning. This may have the same effect as infants&#39; curiosity-based learning by which they shape their own curriculum. However, introducing new goals cannot wait too long, as there may be critical periods in development after which agents (as humans) cannot flexibly learn to incorporate new objectives. These lessons are making their way into machine learning and offer exciting new avenues for studying both human and machine learning of sequential action." />



<link rel="alternate" hreflang="en-us" href="https://kachergis.github.io/publication/deeparm/" />
<link rel="canonical" href="https://kachergis.github.io/publication/deeparm/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu10978985619530458252.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu10391704213710329260.png" />

<meta name="theme-color" content="#bbdefb" />










  
  






<meta property="twitter:card" content="summary" />
<meta property="twitter:image" content="https://kachergis.github.io/media/icon_hu10226378635250344412.png" />



  

<meta property="og:type" content="article" />
<meta property="og:site_name" content="George Kachergis" />
<meta property="og:url" content="https://kachergis.github.io/publication/deeparm/" />
<meta property="og:title" content="A critical period for robust curriculum-based deep reinforcement learning of sequential action in a robot arm | George Kachergis" />
<meta property="og:description" content="Many everyday activities are sequential in nature. That is, they can be seen as a sequence of subactions and sometimes subgoals. In the motor execution of sequential action, context effects are observed in which later subactions modulate the execution of earlier subactions (e.g., reaching for an overturned mug, people will optimize their grasp to achieve a comfortable end state). A trajectory (movement) adaptation of an often-used paradigm in the study of sequential action, the serial response time task, showed several context effects of which centering behavior is of special interest. Centering behavior refers to the tendency (or strategy) of subjects to move their arm or mouse cursor to a position equidistant to all stimuli in the absence of predictive information, thereby reducing movement time to all possible targets. In the current study, we investigated sequential action in a virtual robotic agent trained using proximal policy optimization, a state-of-the-art deep reinforcement learning algorithm. The agent was trained to reach for appearing targets, similar to a serial response time task given to humans. We found that agents were more likely to develop centering behavior similar to human subjects after curricularized learning. In our curriculum, we first rewarded agents for reaching targets before introducing a penalty for energy expenditure. When the penalty was applied with no curriculum, many agents failed to learn the task due to a lack of action space exploration, resulting in high variability of agents&#39; performance. Our findings suggest that in virtual agents, similar to infants, early energetic exploration can promote robust later learning. This may have the same effect as infants&#39; curiosity-based learning by which they shape their own curriculum. However, introducing new goals cannot wait too long, as there may be critical periods in development after which agents (as humans) cannot flexibly learn to incorporate new objectives. These lessons are making their way into machine learning and offer exciting new avenues for studying both human and machine learning of sequential action." /><meta property="og:image" content="https://kachergis.github.io/media/icon_hu10226378635250344412.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2024-10-02T20:54:02&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2022-01-01T00:00:00&#43;00:00">
  






    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://kachergis.github.io/publication/deeparm/"
  },
  "headline": "A critical period for robust curriculum-based deep reinforcement learning of sequential action in a robot arm",
  
  "datePublished": "2024-10-02T20:54:02Z",
  "dateModified": "2022-01-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "R. De Kleijn"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "George Kachergis",
    "logo": {
      "@type": "ImageObject",
      "url": "https://kachergis.github.io/media/icon_hu5609693425197255918.png"
    }
  },
  "description": "Many everyday activities are sequential in nature. That is, they can be seen as a sequence of subactions and sometimes subgoals. In the motor execution of sequential action, context effects are observed in which later subactions modulate the execution of earlier subactions (e.g., reaching for an overturned mug, people will optimize their grasp to achieve a comfortable end state). A trajectory (movement) adaptation of an often-used paradigm in the study of sequential action, the serial response time task, showed several context effects of which centering behavior is of special interest. Centering behavior refers to the tendency (or strategy) of subjects to move their arm or mouse cursor to a position equidistant to all stimuli in the absence of predictive information, thereby reducing movement time to all possible targets. In the current study, we investigated sequential action in a virtual robotic agent trained using proximal policy optimization, a state-of-the-art deep reinforcement learning algorithm. The agent was trained to reach for appearing targets, similar to a serial response time task given to humans. We found that agents were more likely to develop centering behavior similar to human subjects after curricularized learning. In our curriculum, we first rewarded agents for reaching targets before introducing a penalty for energy expenditure. When the penalty was applied with no curriculum, many agents failed to learn the task due to a lack of action space exploration, resulting in high variability of agents' performance. Our findings suggest that in virtual agents, similar to infants, early energetic exploration can promote robust later learning. This may have the same effect as infants' curiosity-based learning by which they shape their own curriculum. However, introducing new goals cannot wait too long, as there may be critical periods in development after which agents (as humans) cannot flexibly learn to incorporate new objectives. These lessons are making their way into machine learning and offer exciting new avenues for studying both human and machine learning of sequential action."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>A critical period for robust curriculum-based deep reinforcement learning of sequential action in a robot arm | George Kachergis</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  dark " data-wc-page-id="c235dea2877add590990a4a6a60b62f5" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.db21e13b9b5c4f4b947717750ec8b3cc.js"></script>

  




  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">George Kachergis</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">George Kachergis</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/kachergis_cv.pdf"><span></span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    


  
    
    
  


<div class="pub">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>A critical period for robust curriculum-based deep reinforcement learning of sequential action in a robot arm</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      R. De Kleijn</span>, <span >
      D. Sen</span>, <span >
      G. Kachergis</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  

  

  
  
  
  

  
  

</div>

    




<div class="btn-links mb-3">
  
  








  



<a class="btn btn-outline-primary btn-page-header" href="/publication/deeparm/deeparm.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/deeparm/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12595" target="_blank" rel="noopener">
    URL</a>


</div>


  
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Many everyday activities are sequential in nature. That is, they can be seen as a sequence of subactions and sometimes subgoals. In the motor execution of sequential action, context effects are observed in which later subactions modulate the execution of earlier subactions (e.g., reaching for an overturned mug, people will optimize their grasp to achieve a comfortable end state). A trajectory (movement) adaptation of an often-used paradigm in the study of sequential action, the serial response time task, showed several context effects of which centering behavior is of special interest. Centering behavior refers to the tendency (or strategy) of subjects to move their arm or mouse cursor to a position equidistant to all stimuli in the absence of predictive information, thereby reducing movement time to all possible targets. In the current study, we investigated sequential action in a virtual robotic agent trained using proximal policy optimization, a state-of-the-art deep reinforcement learning algorithm. The agent was trained to reach for appearing targets, similar to a serial response time task given to humans. We found that agents were more likely to develop centering behavior similar to human subjects after curricularized learning. In our curriculum, we first rewarded agents for reaching targets before introducing a penalty for energy expenditure. When the penalty was applied with no curriculum, many agents failed to learn the task due to a lack of action space exploration, resulting in high variability of agents&rsquo; performance. Our findings suggest that in virtual agents, similar to infants, early energetic exploration can promote robust later learning. This may have the same effect as infants&rsquo; curiosity-based learning by which they shape their own curriculum. However, introducing new goals cannot wait too long, as there may be critical periods in development after which agents (as humans) cannot flexibly learn to incorporate new objectives. These lessons are making their way into machine learning and offer exciting new avenues for studying both human and machine learning of sequential action.</p>
    

    
    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            <a href="/publication/#article-journal">
              Journal article
            </a>
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><em>Topics in Cognitive Science</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    







<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fkachergis.github.io%2Fpublication%2Fdeeparm%2F&amp;text=A&#43;critical&#43;period&#43;for&#43;robust&#43;curriculum-based&#43;deep&#43;reinforcement&#43;learning&#43;of&#43;sequential&#43;action&#43;in&#43;a&#43;robot&#43;arm" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fkachergis.github.io%2Fpublication%2Fdeeparm%2F&amp;t=A&#43;critical&#43;period&#43;for&#43;robust&#43;curriculum-based&#43;deep&#43;reinforcement&#43;learning&#43;of&#43;sequential&#43;action&#43;in&#43;a&#43;robot&#43;arm" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=A%20critical%20period%20for%20robust%20curriculum-based%20deep%20reinforcement%20learning%20of%20sequential%20action%20in%20a%20robot%20arm&amp;body=https%3A%2F%2Fkachergis.github.io%2Fpublication%2Fdeeparm%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fkachergis.github.io%2Fpublication%2Fdeeparm%2F&amp;title=A&#43;critical&#43;period&#43;for&#43;robust&#43;curriculum-based&#43;deep&#43;reinforcement&#43;learning&#43;of&#43;sequential&#43;action&#43;in&#43;a&#43;robot&#43;arm" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=A&#43;critical&#43;period&#43;for&#43;robust&#43;curriculum-based&#43;deep&#43;reinforcement&#43;learning&#43;of&#43;sequential&#43;action&#43;in&#43;a&#43;robot&#43;arm%20https%3A%2F%2Fkachergis.github.io%2Fpublication%2Fdeeparm%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fkachergis.github.io%2Fpublication%2Fdeeparm%2F&amp;title=A&#43;critical&#43;period&#43;for&#43;robust&#43;curriculum-based&#43;deep&#43;reinforcement&#43;learning&#43;of&#43;sequential&#43;action&#43;in&#43;a&#43;robot&#43;arm" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    




  
    




  
    




  
















  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> â€” the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.50933d940896e49f984a778650d5f7f5.js"></script>




  

  
  

  








































<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>










<script src="/en/js/wowchemy.min.57af632a84a05835124f04e4f75dcd29.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js" type="module"></script>


















</body>
</html>
